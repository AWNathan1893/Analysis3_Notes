\documentclass[../Analysis-3.tex]{subfiles}
\myexternaldocument{Lec-12}

\begin{document}
\chapter*{Lecture 13} %Set chapter name
\addcontentsline{toc}{chapter}{Lecture 13} %Set chapter title
\setcounter{chapter}{13} %Set chapter counter
\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{figure}{0}


\section{Taylor's Theorem}\index{Taylor's Theorem}
Recall, Taylor's theorem for one variable.
\begin{Def}{Taylor's Polynomial}{1:taylor:poly}
  Let, $ f : \Op{1} \to \R $ be $C^k \ (k \in \N)$. Then for all $h$ such that $a+h \in \Op{1}$,
  \[ p_{a,k}(a+h) = \sum_{n = 0}^{k} \frac{f^{(n)}(a)}{n!} h^{n}\]
  is called the \textbf{Taylor's Polynomial} of $f$ around $a$.
\end{Def}

\textbf{Question.} ``Is $f(x) \thickapprox  p_{a,k}(x)$, for $x$ close to $a$''?

We have,
\[  p_{a,k}(x) = \sum_{n = 0}^{k} \frac{f^{(n)}(a)}{n!} (x-a)^{n} \]
Take, $f(x) - p_{a,k}(x) = r_{a,k}(x)$

\begin{Thm}{Taylor's Theorem}{taylor}
  Let $ f: \Op{1} \to \R $ be $C^{k+1}$. Then, $f(x) = p_{a,k}(x) + r_{a,k}(x)$ where,
  \[r_{a,k} = \frac{f^{k+1}(c)}{(k+1)!} (x-a)^{k+1}\]
  for some $c$ in between $a$ and $x \in \Op{1}$.
\end{Thm}

We introduce the following notation for the sake of clarity in the multivariate Taylor expansion. Let \( \alpha = (\alpha_1 , \alpha_2 , \cdots , \alpha_n) \in \Z_{\geq 0}^{n} \), and define
\begin{itemize}
  \item $|\alpha| = \sum_{i = 1}^{n} \alpha_i$
  \item $\alpha ! = \alpha_1 ! \alpha_2 ! \cdots \alpha_n !$ \, (product of coordinate-wise factorials)
  \item \( \displaystyle\partial^{\alpha} = \frac{\partial^{|\alpha|}}{\partial {x_1}^{\alpha_1} \cdots \partial {x_n}^{\alpha_n}} \) \, (\( \alpha^{\text{th}} \) derivative)
  \item $\displaystyle\div h = \sum_{i=1}^n h_i\frac{\partial}{\partial x_i}$
\end{itemize}
The last definition when iterated gives,
\[\displaystyle(\div h)^m = \sum_{| \alpha | = m } \frac{m!}{\alpha!} h^{\alpha} \partial^{\alpha}\]


\begin{Thm}{Taylor's Theorem in Multivariate Case}{taylor:multi}
  Let, $ f: \Op{n} \to \R $ be a $C^{k+1}$ function and assume $\Op{n}$ is convex. If $h, a + h \in \Op{n} $, then
  \[ f(a+h) = \sum_{| \alpha | \leq k } \frac{1}{\alpha !} ({\partial}^\alpha f ) (a) h^{\alpha} + r_{a,k} (h) \]
  where, \[ r_{a,k} (h) = \sum_{| \alpha | = k+1 } \frac{1}{\alpha !} ({\partial}^\alpha f ) (a + c h) h^{\alpha} \qquad \text{for some $c \in (0,1)$} \]
\end{Thm}

\begin{proof}
  Define, $ \eta:[0,1] \to \R $ as $ \eta(t) = f(a+th) $
  \[\begin{tikzcd}
      t & {a+th} & {f(a+th)}
      \arrow[maps to, from=1-1, to=1-2]
      \arrow[maps to, from=1-2, to=1-3]
      \arrow["\eta"', curve={height=18pt}, shorten <=6pt, from=1-1, to=1-3]
    \end{tikzcd}\]
  which implies $\eta$ is a $C^{k+1}$ function around $0$.

  \[  \therefore \eta'(t) = \grad f(a+th)\cdot h = (\div h)f(a+th)   \]

  \begin{clmBox}{}
    \[ \eta^{(m)}(t) = (\div h)^m f (a+th) \ \forall\ m \in \qty{0,1,\ldots,k+1} \]
    % where, \[(\div h)^m = \sum_{| \alpha | = m } \frac{m!}{\alpha!} h^{\alpha} \partial^{\alpha} \tag{as notation} \]
  \end{clmBox}
  \begin{proof}
    The first derivative of $\eta$,
    \[  \eta'(t) = \grad f (a+th) \cdot h = \sum_{i=1}^n f_{x_i} (a+th) h_i \]
    which we use to compute the second derivative,
    \begin{align*}
      \eta''(t)
       & = \dv{t}\left(\sum_{i=1}^n f_{x_i} (a+th) h_i\right)                               \\
       & = \sum_{i=1}^n \dv{t}\left( f_{x_i} (a+th) h_i\right)                              \\
       & = \sum_{i=1}^n h_i \sum_{j=1}^n f_{x_ix_j} (a+th) h_j \tag{Chain rule of partials} \\
       & = \sum_{i,j=1}^n h_i h_jf_{x_ix_j} (a+th)                                          \\
       & = (\div h)^2 f (a+th)
    \end{align*}
    Proceeding with induction on the order of the derivative, we get $\eta^{(m)}(t) = (\div h)^m f (a+th)$ for all $0 \leq m \leq k+1$ which is our claim.
  \end{proof}
  By one-variable Taylor's Theorem,
  \begin{equation}\label{eq:taythmineta}
    \eta(1) = p_{0,k}(1) + r_{0,k} (c) \text{   for some $c \in (0,1)$}
  \end{equation}

  with \begin{equation}
    p_{0,k}(1) = \eta(0) + \frac{\eta '(0)}{1!} + \cdots + \frac{\eta^{(k)}(0)}{k!}
  \end{equation} and
  \begin{equation}
    r_{0,k} (c) = \frac{\eta^{(k+1)}(c)}{(k+1)!}
  \end{equation}

  Substituting $\eta^{(m)}(t)$ in (\ref{eq:taythmineta}) we have,
  \[ f(a+h) = \sum_{| \alpha | \leq k } \frac{1}{\alpha !} ({\partial}^{\alpha} f ) (a) h^{\alpha} + r_{a,k} (h) \]
\end{proof}

We note that, in particular, if \( f: \Op{2} \to \R \) is a \( C^2 \) function then we have,
\begin{equation}\label{eq:pj:n2:tp}
  f(a+h) = f(a) + \grad f(a)\cdot h + \frac{1}{2}h^t H_f(a+ch)h
\end{equation} where,
\[ H_f(a) = \begin{pmatrix}
    f_{xx}(a) & f_{xy}(a) \\
    f_{xy}(a) & f_{yy}(a)
  \end{pmatrix} \]
and \( c \in (0,1) \).

\begin{Thm}{Extremum}{}
  Let $f : \Op{2} \to \R$ be a $C^2$ function such that $Df(a) = 0$. We write \[ H_f(a) =  \begin{pmatrix}
      f_{xx}(a) & f_{xy}(a) \\
      f_{xy}(a) & f_{yy}(a)
    \end{pmatrix} \]
  Then, \begin{enumerate}[label=(\roman*)]
    \item $f(a)$ is a local maximum if $f_{xx} (a) < 0$ and $\det (H_f(a)) > 0$
    \item $f(a)$ is a local minimum if $f_{xx} (a) > 0$ and $\det (H_f(a)) > 0$ \label{extremum:2}
    \item $a$ is a saddle point if $\det (H_f(a)) < 0$
  \end{enumerate}
\end{Thm}

\begin{proof}
  As $ a $ is an interior point of $ \Op{2} $ we can get an $ r > 0 $ such that $ a, a+h \in B_r(a) \subseteq \Op{2}$. By $\eqref{eq:pj:n2:tp}$, \[ f(a+h) - f(a) = \grad f(a) \cdot h + \frac{1}{2}h^t H_f(a+ch)h \]

  We will prove \ref{extremum:2}, other statements can be proved similarly. Our assumptions tell that $H_f(a)$ is positive definite. Hence, by the Lemma \ref{lem:hessian:nbd}, there exist $ \epsilon > 0 $ such that $H_f(x)$ is positive definite $\forall\ x \in B_{\epsilon}(a)$. So for every $ x \in B_{\epsilon}(a) $ the quadratic form $ h^t H_f(x) h > 0 $ with $ h \neq 0 $ which implies $ f(x) - f(a) > 0 $ in $ B_{\epsilon}(a) $ that means $a$ is a point of local minimum.
\end{proof}

\begin{Eg}{Finding Critical points of a function and their nature}{}
  Find the critical points and discuss the nature of the function
  \[ f(x,y) = x^3 - 6x^2 - 8y^2 \]

  \textit{Solution. } Setting $ \grad f(x,y) = 0 $, i.e., $ (f_x,f_y)(x,y) = 0 $, we get the system of equations
  \[ 3x^2 - 12x = 0 \text{  and  }  -16y = 0\]
  whose solution set is $ \qty{(0,0),(4,0)} $ implying that $(0,0), (4,0)$ are critical points.

  The 2nd derivatives are, \[ f_{xx}(x,y) = 6x - 12,\ f_{yy}(x,y) = -16 \text{ and } f_{xy}(x,y) = 0 \]
  Now, we compute the determinant of the hessian at these points to tell their nature. For $(0,0)$,
  \[ \det(H_f(0,0)) =  \begin{vmatrix}
      -12   & \ \ 0 \\
      \ \ 0 & -16
    \end{vmatrix} > 0 \text{  and  } f_{xx}(0,0) = -12 < 0\]
  So, $f$ has a local maximum at $(0,0)$. And at $(4,0)$,
  \[ \det(H_f(4,0)) =  \begin{vmatrix}
      12 & \ \ 0 \\
      0  & -16
    \end{vmatrix} < 0 \]
  which shows that $(4,0)$ is a saddle point. $\hfill \blacksquare$
\end{Eg}


\end{document}
