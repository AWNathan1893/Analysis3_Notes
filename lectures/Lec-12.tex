\documentclass[../Analysis-3.tex]{subfiles}

\begin{document}
\chapter*{Lecture 12} %Set chapter name
\addcontentsline{toc}{chapter}{Lecture 12} %Set chapter title
\setcounter{chapter}{12} %Set chapter counter
\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{figure}{0}


\section{Hessian Matrix}

We start by defining Hessian matrix, which is a natural extension of the concept of the second derivative in higher dimensions, allowing us to analyze the rate of change and curvature of a function in multiple directions simultaneously.

\begin{Def}{Hessian}{hessian}
  Suppose $f : \Op{n} \to \R$ is $C^2$ at $a \in \Op{n} $. The $\textbf{Hessian of $f$ at $a$}$ is defined as the matrix,
  \[ H_f(a) = \left( \pdv{f}{x_i}{x_j}\left( a \right) \right)_{n \times n} \]
\end{Def}

In explicit notation, it has the following form,

\[ H_{f} = \begin{pmatrix}
    \pdv[2]{f}{x_1}   & \pdv{f}{x_1}{x_2} & \ldots & \pdv{f}{x_1}{x_n} \\
    \pdv{f}{x_2}{x_1} & \pdv[2]{f}{x_2}   & \ldots & \pdv{f}{x_2}{x_n} \\
    \vdots            & \vdots            & \ddots & \vdots            \\
    \pdv{f}{x_n}{x_1} & \pdv{f}{x_n}{x_2} & \ldots & \pdv[2]{f}{x_n}
  \end{pmatrix} \]

It is important to note that for any function $f$ that is twice continuously differentiable ($f \in C^2$), its Hessian matrix $H_f$ is symmetric, meaning that $H_f = {H_f}^t$.

\begin{Eg}{}{}
  Let, $f : \R^2 \to \R$ be a function defined by $f(x,y) = \sin^2 x + x^2y + y^2$. Then,
  \[  Df = \begin{pmatrix}
      \sin2x + 2xy & x^2 + 2y
    \end{pmatrix} :  \R^2 \to \R \quad \text{is linear.}  \]

  The gradient is given by, \[  \grad f = \left\langle \sin2x + 2xy, x^2 + 2y \right\rangle \in \R^2  \]

  And the Hessian matrix $H_f$ is,
  \[ H_f = \begin{pmatrix}
      f_{xx} & f_{xy} \\
      f_{xy} & f_{yy}
    \end{pmatrix} = \begin{pmatrix}
      2( \cos2x + y) & 2x \\
      2x             & 2
    \end{pmatrix} \quad \left[ \because  f \in C^2 \right] \]

\end{Eg}

\textbf{Now let's introduce some notation.} Given $A = \left( a_{ij} \right)_{n\times n} \in M_n(\R)$ and $ x \in \R^n $ we denote $ Q_A(x) $  by,

\begin{align*}
  Q_A(x) = x^tAx
   & = {\left\langle Ax,x \right\rangle}_{\R^n}
  \\
   & = \begin{pmatrix}
         x_1 & x_2 & \cdots & x_n
       \end{pmatrix} \begin{pmatrix}
                       a_{11} & a_{12} & \cdots & a_{1n} \\
                       a_{21} & a_{22} & \cdots & a_{2n} \\
                       \vdots & \vdots & \ddots & \vdots \\
                       a_{31} & a_{32} & \cdots & a_{3n}
                     \end{pmatrix} \begin{pmatrix}
                                     x_1 \\ x_2 \\ \vdots \\ x_n
                                   \end{pmatrix} \\
   & = \sum_{i=1}^{n}\sum_{j=1}^{n} a_{ij} x_i x_j
\end{align*}


\begin{Def}{Quadratic Form}{quadform}
  A function $f : \R^n \to \R$ is called a \textbf{Quadratic Form} if it can be expressed as $f(x) = Q_A(x)$ for all $x$ and for some symmetric $A \in M_n(\R)$
\end{Def}

It is important to note that a Quadratic Form represents a homogeneous polynomial of degree 2.

For instance, in the case of a bivariate polynomial $p(x,y) = a_{11} x^2 + a_{22} y^2 + a_{12} xy$, the matrix \[ A = \begin{pmatrix}
    a_{11}             & \frac{1}{2} a_{12} \\
    \frac{1}{2} a_{12} & a_{22}
  \end{pmatrix} \] corresponds to the quadratic form $p(x,y) = Q_A(x,y)$, capturing the essential quadratic behavior of $p$.

\section{Positive Definite, Negative Definite, Semi Definite Matrices}

\begin{Def}{Positive Definite, Negative Definite, Semi Definite}{pd:nd:sd}
  \begin{itemize}
    \item A symmetric matrix $A \in M_n(\R)$ is called \textbf{Positive Definite} if \[ \left\langle Ax, x \right\rangle > 0 \ \forall\ x \in \R^n \setminus \{0\} \]
    \item A symmetric matrix $A \in M_n(\R)$ is called \textbf{Negative Definite} if \[ \left\langle Ax, x \right\rangle < 0 \ \forall\ x \in \R^n \setminus \{0\} \]
    \item A symmetric matrix $A \in M_n(\R)$ is called \textbf{Semi Definite} if \[ \left\langle Ax, x \right\rangle \geq 0 \ \forall\ x \in \R^n \setminus \{0\} \]
  \end{itemize}
\end{Def}

\begin{Eg}{}{}
  \begin{enumerate}
    \item  $I_n$ is positive definite because for any vector $x \in \R^n \setminus \{0\}$ the inner product \[ \left\langle I_nx, x \right\rangle = \norm{x}^2 > 0 \] is strictly positive.
    \item For any matrix $A \in M_n(\R)$, if there exists a matrix $B \in M_n(\R)$ such that $A = B^tB$, we can examine the inner product $\langle Ax, x \rangle$ for any $x \in \R^n \setminus \{0\}$.

          Let's compute this inner product
          \begin{align*}
            \langle Ax, x \rangle
             & = \langle B^tBx, x \rangle                                      \\
             & = x^tB^tBx                                                      \\
             & = (Bx)^t(Bx)                                                    \\
             & = \norm{Bx}^2 \quad \text{for all } x \in \R^n \setminus \{0\}.
          \end{align*}

          Therefore, we conclude that $\langle Ax, x \rangle \geq 0$ for all $x \in \R^n \setminus \{0\}$.

          Moreover, if $\langle Ax, x \rangle = 0$, then $Bx = 0$, which implies that $x$ is in the kernel of $B$. Conversely, if $A$ is positive definite, there is no non-zero vector $x$ such that $\langle Ax, x \rangle = 0$. This implies that the columns of $B$ are linearly independent.

          In summary, when $A$ can be written as $A = B^tB$ for some matrix $B$, we can conclude that $A$ is positive semi-definite (and the converse also holds).

    \item Consider the matrix \[ A = \begin{pmatrix}
              1 & 0  \\
              0 & -1
            \end{pmatrix} \]
          We can compute the quadratic form associated with $A$ as $Q_A = x_1^2 - x_2^2$.

          By examining this expression, we observe that it is the difference between the squares of two variables. This indicates that the sign of $Q_A$ can change depending on the values of $x_1$ and $x_2$. Consequently, the matrix $A$ is considered \textbf{indefinite}.
    \item Consider the matrix \[ \begin{pmatrix}
              1 & 0 \\
              0 & 0
            \end{pmatrix} \]
          To determine the definiteness of $A$, we can compute the quadratic form associated with $A$ as $Q_A(x) = x_1^2 \geq 0$ for all $x = \begin{pmatrix} x_1 & x_2 \end{pmatrix}$. This indicates that the matrix $A$ is \textbf{positive semi-definite}.

  \end{enumerate}
\end{Eg}

Consider a positive definite matrix $A$. For any non-zero vector $h$, we have,
\[
  \left\langle Ah, h \right\rangle = \norm{Ah}\norm{h}\cos{\theta} > 0
\]
where $\theta$ is the angle between vectors $Ah$ and $h$. Since the cosine of any angle $\theta$ in the interval $[0, \pi/2)$ is positive, we can conclude that,
\[
  \cos{\theta} > 0 \implies \boxed{0 \leq \theta < \frac{\pi}{2}}
\]

Thus, for any positive definite matrix $A$, the angle $\theta$ between $Ah$ and $h$ satisfies $0 \leq \theta < \frac{\pi}{2}$.

\

It is worth noting that classifying positive definite matrices becomes more challenging for higher dimensions ($n > 2$). However, for $2 \times 2$ matrices, we can easily determine it from the next theorem.

\begin{Thm}{}{PdNdInd:char}
  Let $A = \begin{pmatrix}
      a & b \\
      b & c
    \end{pmatrix} \in M_2(\R) $ be symmetric. Then,
  \begin{enumerate}[label=(\roman*)]
    \item $A$ is \textbf{Positive Definite} $ \Longleftrightarrow a > 0 \text{ and } ac -b^2 >0 $ \label{charpd}
    \item $A$ is \textbf{Negative Definite} $ \Longleftrightarrow a < 0 \text{ and } ac -b^2 >0 $ \label{charnd}
    \item $A$ is \textbf{Indefinite} $ \Longleftrightarrow ac -b^2 <0 $ \label{charind}
  \end{enumerate}
\end{Thm}

\begin{proof}
  We have $\langle Ah, h \rangle = h^tAh$ for any vector $h$. Now, consider a non-zero vector ${\bf x} = (x_1, x_2) \in \R^2 \backslash \{(0,0)\}$ with $x_2 \neq 0$. Without loss of generality, we can scale ${\bf x}$ as ${\bf x} = (x,1)$ for some $x \in \R$. Then, we have,
  \[ \inp{A\bf{x}}{\bf{x}} = ax^2 + 2bx + c > 0 \ \forall\ x \in \R \]

  If $x_2 = 0$, we can choose ${\bf x} = \begin{pmatrix}
      1 \
      0
    \end{pmatrix}$ (after scaling). Then, $\langle A{\bf x}, {\bf x} \rangle = a$. Therefore, we can summarize the conditions as follows,
  \begin{align*}
                        & \ A \text{ is Positive Definite}                         \\
    \Longleftrightarrow & \ a > 0 \text{ and } ax^2 + bx+c > 0 \ \forall\ x \in \R \\
    \Longleftrightarrow & \ a > 0 \text{ and } (2b)^2 - 4ac < 0                    \\
    \Longleftrightarrow & \ a > 0 \text{ and } ac - b^2 > 0
  \end{align*}

  Similarly, we can derive the conditions for negative definite and indefinite matrices as,
  \begin{align*}
                        & \ A \text{ is Negative Definite}                         \\
    \Longleftrightarrow & \ a < 0 \text{ and } ax^2 + bx+c < 0 \ \forall\ x \in \R \\
    \Longleftrightarrow & \ a < 0 \text{ and } (2b)^2 - 4ac < 0                    \\
    \Longleftrightarrow & \ a < 0 \text{ and } ac - b^2 > 0
  \end{align*}

  And finally for indefinite ones,
  \begin{align*}
                        & \ A \text{ is Indefinite}                                      \\
    \Longleftrightarrow & \ ax^2 + bx+c < 0 \ \text{for some } x \in \R                  \\
                        & \quad \text{ and } ax^2 + bx+c > 0 \ \text{for some } x \in \R \\
    \Longleftrightarrow & \ (2b)^2 - 4ac > 0                                             \\
    \Longleftrightarrow & \ ac - b^2 < 0
  \end{align*}
\end{proof}

\begin{Lem}{}{hessian:nbd}
  Let, $a \in \Op{n},\ A(x) = \begin{pmatrix}
      a_1(x) & a_2(x) \\
      a_2(x) & a_3(x)
    \end{pmatrix}$. Suppose, $A$ is continuous at $a$ (i.e., $a_i$'s are continuous at $a$). Then, $A$ is Positive Definite at $a$ would imply that $A$ is Positive Definite in a neighborhood of $a$.
\end{Lem}

\begin{proof}
  $ A(a) $ is Positive Definite, i.e., $ a_1(a) > 0 $ and $ a_1(a) a_3(a) - {a_2}^2(a) > 0 $. As $ a_1(x) $ and $ a_1(x)a_3(x) - a_2^2(x) $ are polynomial of continuous functions, we can find an $ \epsilon > 0 $ such that both are positive in $ B_{\epsilon}(a) $.
\end{proof}


\end{document}
