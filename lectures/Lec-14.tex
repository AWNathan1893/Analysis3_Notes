\documentclass[../Analysis-3.tex]{subfiles}

\begin{document}
\chapter*{Lecture 14} %Set chapter name
\addcontentsline{toc}{chapter}{Lecture 14} %Set chapter title
\setcounter{chapter}{14} %Set chapter counter
\setcounter{section}{0}

\section{Compact subsets of $ \R^n $}

We start with the definition of Compactness which refers to a property of sets that captures the notion of being finite or having no ``holes''.
\begin{Def}{Compact Subset}{}
  A subset $ K \subseteq \R^n $ is said to be compact if every sequence $ \{x_n\} \subseteq K $ has a subsequence $ \{x_{n_k}\} $ that is convergent to some $ x \in K $.
\end{Def}

This is known as the Bolzano-Weierstrass Property.

\

Observe that a compact subset of $ \R^n $ is always \textbf{closed}. To see this, note that every sequence $ \{x_n\} \subseteq K $, where $ K $ is a compact subset of $ \R^n $ that converges to some $ x \in \R^n $ has a convergent subsequence $ \left\{x_{n_k}\right\} $ that converges to the same $ x $. Since $ K $ is compact, we can say that $ x \in K $. So, the convergent sequence $ \{x_n\} $ converges to a point in $ K $. Hence, $ K $ is closed.

\

More is true. A compact subset of $ \R^n $ is \textbf{bounded} too. Assume that a compact subset $ K \subseteq \R^n $ is not bounded. Note that, a subset of $ \R^n $ is bounded iff it is contained inside an open ball. Since $ K $ is unbounded, we can get a sequence $ \left\{ x_m \right\} \subseteq K $ with $ \norm{x_m} > m $, which doesn't have a convergent subsequence. This shows that $ K $ is not compact that contradicts our assumption.

\

Therefore, a compact subset of $ \R^n $ is closed and bounded. What about the converse?

\begin{Thm}{}{clbox:cpct}
  A closed and bounded box in $ \R^n $ is compact.
\end{Thm}

\begin{proof}
  We take a closed and bounded box $ \displaystyle K := \prod_{i=1}^{n} [a_i, b_i] \subseteq \R^n $. Fix $ i \in [n] $. Consider a sequence $ \left\{ x_m \right\} \subseteq K $. We take its projection on the $ i^{\text{th}} $ coordinate, i.e., $ \left\{ \Pi_i(x_m) \right\} \subseteq [a_i, b_i] $. By Bolzano-Weierstrass Theorem, it has a convergent subsequence $ \left\{ \Pi_i(x_m)_t \right\} \subseteq [a_i, b_i] $ converging to $ \alpha_i \in [a_i, b_i] $. Consider the intersection of inverse images of all such convergent subsequences, $ \displaystyle \bigcap_{i=1}^n \Pi_i^{-1}\left\{ \Pi_i(x_m)_t \right\} $, which is a convergent subsequence of $ \left\{ x_m \right\} $ converging to $ \alpha = (\alpha_1, \ldots, \alpha_n) \in K $.
\end{proof}



\begin{Thm}{Heine-Borel Theorem}{heine:borel}
  A subset $ K \subseteq \R^n $ is compact iff it is closed and bounded.
\end{Thm}

\begin{proof}
  \begin{itemize}
    \item[$ \Longrightarrow $] Done!
    \item[$ \Longleftarrow $] Since, $ K $ is bounded, it is contained in a closed box, i.e., there exists $ r > 0 $ such that $ K \subseteq [-r, r]^n $. So, Theorem \ref{th:clbox:cpct} implies that all sequences in $ K $ has a convergent subsequence, which must converge in $ K $ because $ K $ is closed. Hence, $ K $ is compact.
  \end{itemize}
\end{proof}



\begin{Thm}{}{cpct:cts:cpct}
  Let $ f:\Op{n} \to \R^m $ be a continuous map. Then $ f $ sends compact sets to compact sets.
\end{Thm}

In other words, continuous image of a compact set is compact.

\begin{proof}
  Let $ K \in \Op{n} $ be compact. Take a sequence $ \left\{ x_k \right\} \subseteq K $ with a convergent subsequence $ \left\{ x_{k_t} \right\} \subseteq K $ converging to $ x \in K $. Then, $ \left\{ f(x_k) \right\} $ is sequence in $ f(K) $  with convergent subsequence $ \left\{ f(x_{m_t}) \right\} $  converging to $ f(x) $. The last statement about convergence follows from the continuity of $ f $. This shows that $ f(K) $ is compact.
\end{proof}


\begin{Thm}{Extreme Value Theorem}{ext:val}
  Let $ K \subseteq \R^n $ be compact and $ f:K \to \R $ a continuous map. Then $\exists\ a,b \in K $ such that $ f(a) \leq f(x) \leq f(b) $ for all $ x \in K $.
\end{Thm}

\begin{proof}
  By Theorem \ref{th:cpct:cts:cpct}, $ f(K) $ is compact. So $ f $ is bounded which implies $ \displaystyle \sup_K f, \inf_K f $ exist! Since $ f(K) $ is closed, they must exist inside $ f(K) $.
\end{proof}

\section{Inverse Function Theorem}

We are now going to study Inverse Function Theorem which relates the differentiability of a function to the differentiability of its inverse, enabling the study of local behavior and solving equations in higher dimensions. But before that we prove a lemma that is essential to prove the theorem.

\begin{Lem}{}{sup:diff}
  Let $ \Op{n} \subseteq \R^n $ be open and convex. Suppose $ f:\Op{n} \to \R^n $ be a $ C^1 $ function. If $ \exists\ M > 0 $ such that
  \[ \sup_{x \in \Op{n}} \abs{\pdv{f_i}{x_j}\left( x \right)} \leq M \text{ for all } i, j \]
  Then $ \norm{f(x) - f(y)} \leq n^2 M\norm{x-y} $ for every $ x, y \in \Op{n} $.
\end{Lem}

\begin{proof}
  Pick $ x, y \in \Op{n} $ and $ i \in [n] $. Then using Mean Value Theorem, we can get $ c_i \in L_{x, y} $ such that
  \begin{align*}
    f_i(x) - f_i(y)
     & = \grad f_i (c_i) \cdot (x - y)                                                                     \\
    \implies \abs{f_i(x) - f_i(y)}
     & = \abs{\sum_{j=1}^{n} \pdv{f_i}{x_j}\left( c_i \right) \cdot (x_i - y_i)}                           \\
     & \leq \sum_{j=1}^{n} \abs{\pdv{f_i}{x_j}\left( c_i \right)}\abs{x_i - y_i} \tag{Triangle inequality} \\
     & \leq M \sum_{j=1}^{n} \abs{x_i - y_i} \leq nM \norm{x-y}
  \end{align*}
  The last inequality follows from the inequality $ \abs{x_i-y_i} \leq \norm{x-y} $ which holds for all $ i $.

  Using the above, \[ \norm{f(x) - f(y)} = \sqrt{\sum_{i=1}^{n} \abs{f_i(x) - f_i(y)}^2} \leq \sqrt{\sum_{i=1}^{n} n^2M^2\norm{x-y}^2 } \leq n^2 M\norm{x-y} \]
  we obtain the result.
\end{proof}


\begin{Thm}{Inverse Function Theorem}{inv:fun}
  Let $ f:\Op{n} \to \R^n $ be a $ C^1 $ function and $ a \in \Op{n} $. Suppose $ Df(a) $ is invertible. Then, there exist open sets $ V $ and $ W $ containing $ a $ and $ f(a) $ respectively, such that $ f: V \to W $ is invertible.

  \;

  Moreover, the local inverse $ f^{-1} \equiv \left( f\vert_{{}_V} \right)^{-1} : W \to V $ is differentiable and for all $ y \in W $,
  \[ Df^{-1}(y) = \left( \left( Df \right)\left( f^{-1}(y) \right) \right)^{-1} \]
  i.e., locally, the derivative of the inverse is the matrix inverse of the derivative.
\end{Thm}

\begin{proofFig}{\subincfig{inverseFunction}}{}{\label{fig:inf:fun}}{13}{.4\textwidth}
  We call $ L = Df(a) $ which is given to be invertible and take $ g(x) := L^{-1}f(x) $. Then,
  \begin{align*}
    Dg(a)
     & = L^{-1}(f(a))\cdot (Df)(a)            \\
     & = \left[ L^{-1} \right]\cdot Df(a) = I
  \end{align*}

  As this transformation can be made, without loss of generality, we may assume that $ Df(a) = I_n $ which would imply that there exists a closed box $ U $ containing $ a $ such that for all $ x\in U\setminus\{a\}, f(a) \not= f(x) $. To see this, let $ f(a) = f(a+h) $ with arbitrarily small $ \norm{h} $. But then,
  \[  \frac{1}{\norm{h}}\left( f(a+h)-f(a)-Ih \right) = \frac{h}{\norm{h}} \not= 0  \]
  which contradicts the definition of derivative. Note that $ \det J_f(a) \not= 0 $. So, by continuity $ \det J_f(x) \not= 0 $ for all $ x \in U $ (we may shrink $ U $ if necessary). Hence, $ Df(x) $ is invertible for all $ x \in U $. Again by continuity, for all $ x \in U $ (we may shrink $ U $ if necessary),
  \[  \Bigg\vert \pdv{f}{x_j}\left( x \right) - \underbrace{\pdv{f}{x_j}\left( a \right)}_{\delta_{ij}} \Bigg\vert \leq \frac{1}{2n^2}  \]

  Now we claim the following,
  \begin{clmBox}
    For all $ x, y \in U $,
    \[  \norm{f(x)-f(y)} \geq \frac{1}{2}\norm{x-y}  \]
  \end{clmBox}

  \begin{proof}
    We take $ g(x) = f(x) - x $ for all $ x \in U $. Taking derivative, we get
    \begin{align*}
      Dg(x)    & = Df(x) - I  = Df(x) - Df(a)                                                                     \\
      \implies & \pdv{g_i}{x_j}\left( x \right) = \pdv{f_i}{x_j}\left( x \right) - \pdv{f_i}{x_j}\left( a \right) \\
      \implies & \abs{\pdv{g_i}{x_j}\left( x \right)} \leq \frac{1}{2n^2} \ \forall\ x \in U
    \end{align*}

    Then by Lemma \ref{lem:sup:diff}, for all $ x, y \in U $,
    \begin{align}
               & \norm{g(x)-g(y)} \leq n^2\cdot\frac{1}{2n^2}\norm{x-y} \nonumber                          \\
      \implies & \norm{(f(x)-f(y)) - (x-y)} \leq \frac{1}{2}\norm{x-y}  \nonumber                          \\
      \implies & \boxed{\norm{f(x)-f(y)} \geq \frac{1}{2}\norm{x-y}} \label{eqn:clm:normdiff:ge:th:14.2.1}
    \end{align}

    The last line follows from the Triangle inequality. So, we get the claim! It shows that $ f $ is injective.
  \end{proof}

  Next we look at the compact set $ \partial U \subseteq U $. Since $ a \not\in \partial U $, we can say $ f(x) \not= f(a) $ for all $ x \in \partial U $. So, by continuity of $ f $ and compactness of $ \partial U $,  we can find a $ d \in \R_{\geq 0} $ such that
  \[  \norm{f(x)-f(a)} \geq d \quad \forall\ x \in \partial U \]

  \begin{figure}[h]
    \centering
    \subincfig{boundaryU_invfun}
    \caption{}
    \label{fig:boundaryU_invfun}
  \end{figure}

  Now, we take $ W = B_{\frac{d}{2}}\left( f(a) \right) $. Then, for every $ y \in W $ and $ x \in \partial U $,
  \begin{equation}\label{eqn:normdist:ytofa:ytofx:th:14.2.1}
    \underbrace{\norm{y-f(a)}}_{\text{atmost } \frac{d}{2}} < \underbrace{\norm{y-f(x)}}_{\text{atleast } d}
  \end{equation}


  \begin{clmBox}
    For a fixed $ y \in W, \exists$ a unique $ x_0 \in U^{\circ} $ such that $ f(x_0) = y $
  \end{clmBox}

  \begin{proof}
    We define a continuous function $ g:U \to \R $ with \[ g(x) = \norm{y-f(x)}^2 = \sum_{i=1}^{n}\left( y_i - f_i(x) \right)^2 \]

    Since, $ \inf_U g $ cannot occur at the boundary $ \partial U $, but must occur in $ U $, there exists $ x_0 \in U^{\circ} $ such that
    \[\grad g(x_0) = 0 \text{ i.e., } \pdv{g}{x_j}\left( x_0 \right) = 0 \ \forall j \]
    Now, the partials of $ g $,
    \begin{align*}
      \pdv{g}{x_j}\left( x \right)
       & = \pdv{x_j} \sum_{i=1}^{n}\left( y_i - f_i(x) \right)^2                     \\
       & = -2\sum_{i=1}^{n}\left( y_i - f_i(x) \right)\pdv{f_i}{x_j}\left( x \right)
    \end{align*}
    At $ x_0 $, we get,
    \begin{align*}
      \sum_{i=1}^{n}\left( y_i - f_i(x_0) \right)\pdv{f_i}{x_j}\left( x_0 \right) & = 0 \ \forall j          \\
      \implies \underbrace{\begin{pmatrix}
                               \pdv{f_i}{x_j}\left( x_0 \right)
                             \end{pmatrix}^t}_{Df(x_0)^t} \left( y-f_i(x_0) \right)                      & = 0
    \end{align*}
    As $ Df $ is invertible in $ U $ and $ x_0 \in U^{\circ} $ we obtain $ y = f(x_0) $, which shows the existence of $ x_0 $. The uniqueness follows from (\ref{eqn:clm:normdiff:ge:th:14.2.1}).
  \end{proof}

  We now set $ V := U \cap f^{-1}(W) $. Since $ U $ is closed, $ V = U^{\circ} \cap f^{-1}(W) $. Hence, $ f\vert_{{}_V}: V \to W $ is invertible!


  \begin{clmBox}
    $ f^{-1} \equiv \left( f\vert_{{}_V} \right)^{-1}:W \to V $ is continuous.
  \end{clmBox}

  \begin{proof}
    (\ref{eqn:clm:normdiff:ge:th:14.2.1}) gives,
    \[  \norm{f(x_1) - f(x_2)} \geq \frac{1}{2}\norm{x_1 - x_2} \ \forall\ x_1,x_2 \in V \subseteq U  \]
    equivalently,
    \[  2\norm{y_1 - y_2} \geq \norm{f^{-1}(y_1) - f^{-1}(y_2)}   \tag{where, $ y_i = f(x_i) $}  \]
    which shows that $ f^{-1} $ is Lipschitz, hence continuous.
  \end{proof}


  \begin{clmBox}
    $ f^{-1} $ is differentiable.
  \end{clmBox}

  \begin{proof}
    We fix $ y_0 = f(x_0) \in W $ for some $ x_0 \in V $ and take $ A = Df(x_0) $. As we know,
    \begin{align}
      \lim_{h \to 0}                       & \frac{1}{\norm{h}}\left( f^{-1}(y_0+h) - f^{-1}(y_0) - A^{-1}h \right)              = 0  \nonumber                  \\
      \Longleftrightarrow \lim_{y \to y_0} & \frac{1}{\norm{y-y_0}}\left( f^{-1}(y) - f^{-1}(y_0) - A^{-1}(y-y_0) \right) = 0 \label{eqn:clm:lim:finv:th:14.2.1}
    \end{align}

    We set $ \phi(h) = f(x_0+h) - f(x_0) - Ah $ for $ h $ in a neighborhood of $ 0 $. Now,
    \begin{align*}
      A^{-1}\left( f(x_0+h) - f(x_0) \right)
       & = h + A^{-1}\phi(h)                                          \\
       & = ((x_0+h) - x_0) + A^{-1}\left( \phi((x_0+h) - x_0) \right)
    \end{align*}

    Also set $ y = f(x_0+h) $. Then,
    \begin{align}
               & A^{-1}(y-y_0) = f^{-1}(y) - f^{-1}(y_0) + A^{-1}\left( \phi\left(f^{-1}(y) - f^{-1}(y_0)\right) \right)  \nonumber                                              \\
      \implies & -A^{-1}\left( \phi\left(f^{-1}(y) - f^{-1}(y_0)\right) \right) = f^{-1}(y) - f^{-1}(y_0) - A^{-1}(y-y_0)  \label{eqn:clm:limtermintermsofAinvphifinv:th:14.2.1}
    \end{align}

    So, it is now enough to prove that,
    \begin{align*}
      \lim_{h \to 0}                       & \frac{1}{\norm{h}}A^{-1}\left( \phi\left(f^{-1}(y) - f^{-1}(y_0)\right) \right)              = 0 \\
      \Longleftrightarrow \lim_{y \to y_0} & \frac{1}{\norm{y-y_0}}\left( \phi\left(f^{-1}(y) - f^{-1}(y_0)\right) \right) = 0
    \end{align*}

    But,
    \[  \frac{\norm{\phi\left(f^{-1}(y) - f^{-1}(y_0)\right)}}{\norm{y-y_0}} = \underbrace{\frac{\norm{\phi\left(f^{-1}(y) - f^{-1}(y_0)\right)}}{\norm{f^{-1}(y) - f^{-1}(y_0)}}}_{0} \cdot \underbrace{\frac{\norm{f^{-1}(y) - f^{-1}(y_0)}}{\norm{y-y_0}}}_{\leq 2} = 0  \]

    Hence, the limit (\ref{eqn:clm:lim:finv:th:14.2.1}) is true!
  \end{proof}
  Also, $ f^{-1} $ is $ C^1 $, since all partials of $ f^{-1} $ are rational polynomial functions (with non-zero denominators) of those of $ f $. And that completes the proof.
\end{proofFig}

\end{document}